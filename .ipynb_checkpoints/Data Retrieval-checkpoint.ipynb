{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Reddit Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieving data from reddit via the Pushshift API.\n",
    "\n",
    "Approach is based on O'Brien, 2020:\n",
    "\n",
    "https://github.com/iterative/aita_dataset\n",
    "\n",
    "DOI: 10.5281/zenodo.3677563"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Loading required modules\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Defining the function to retrieve data from the API\n",
    "def getPushshiftData(after, before):\n",
    "    url = 'https://api.pushshift.io/reddit/submission/search/'\n",
    "    sort = '?sort_type=created_utc&sort=asc'\n",
    "    subr = '&subreddit=Bitcoin'\n",
    "    after = '&after=' + str(after)\n",
    "    before = '&before=' + str(before)\n",
    "    size = '&size=100'\n",
    "    full_url = url + sort + subr + after + before + size\n",
    "    print(full_url)\n",
    "    r = requests.get(full_url)\n",
    "    data = json.loads(r.text)\n",
    "    return data['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Defining the function to put raw data into a dataset\n",
    "def scraping(first_epoch, last_epoch, filename):\n",
    "    timestamps = []\n",
    "    authors = []\n",
    "    scores = []\n",
    "    comments = []\n",
    "    ids = []\n",
    "    titles = []\n",
    "    texts = []\n",
    "\n",
    "    after = first_epoch\n",
    "    while int(after) < last_epoch:\n",
    "        data = getPushshiftData(after,last_epoch)\n",
    "        tmp_times = []; tmp_authors = []; tmp_scores = [];\n",
    "        tmp_coms = []; tmp_ids = []; tmp_titles = [];\n",
    "        tmp_texts = [];\n",
    "\n",
    "        for post in data:\n",
    "            tmp_times.append(post['created_utc'])\n",
    "            tmp_authors.append(post['author'])\n",
    "            tmp_scores.append(post['score'])\n",
    "            tmp_coms.append(post['num_comments'])\n",
    "            tmp_ids.append(post['id'])\n",
    "            tmp_titles.append(post['title'])\n",
    "            try:\n",
    "                tmp_texts.append(post['selftext'])\n",
    "            except:\n",
    "                tmp_texts.append(math.nan)\n",
    "\n",
    "        try:\n",
    "            if max(tmp_times) not in timestamps:\n",
    "                timestamps = timestamps + tmp_times\n",
    "                authors = authors + tmp_authors\n",
    "                scores = scores + tmp_scores\n",
    "                comments = comments + tmp_coms\n",
    "                ids = ids + tmp_ids\n",
    "                titles = titles + tmp_titles\n",
    "                texts = texts + tmp_texts\n",
    "            else:\n",
    "                break\n",
    "        except:\n",
    "            break\n",
    "\n",
    "\n",
    "        after = max(timestamps)\n",
    "\n",
    "        print([str(len(ids)) + \" posts collected so far.\"])\n",
    "        time.sleep(3)\n",
    "\n",
    "    # Write to a csv file\n",
    "    d = {'id':ids, 'timestamp':timestamps, 'author':authors,\n",
    "        'score':scores, 'comments':comments, # 'sticks':sticks,\n",
    "        'title':titles, 'text':texts}\n",
    "    df = pd.DataFrame(d)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Calling the function to retrieve the data and save.\n",
    "# Note: In order not to overuse the API, the entire time period\n",
    "# is split up in different subsets\n",
    "# Note 2: The files will subfiles are not in the GitHub repo because\n",
    "# of the .gitignore.\n",
    "\n",
    "scraping(1451606400, 1467331200, filename=\"redditjanjun2016.csv\")     # 1\n",
    "scraping(1467331200, 1483228800, filename=\"redditjuldec2016.csv\")     # 2\n",
    "\n",
    "scraping(1483228800, 1498867200, filename=\"redditjanjun2017.csv\")     # 3\n",
    "scraping(1498867200, 1514764800, filename=\"redditjuldec2017.csv\")     # 4\n",
    "\n",
    "scraping(1514764800, 1528963814, filename=\"redditjanjun2018a.csv\")     # 5a\n",
    "scraping(1528963814, 1530403200, filename=\"redditjanjun2018b.csv\")     # 5b\n",
    "scraping(1530403200, 1546300800, filename=\"redditjuldec2018.csv\")     # 6\n",
    "\n",
    "scraping(1546300800, 1561939200, filename=\"redditjanjun2019.csv\")     # 7\n",
    "scraping(1561939200, 1577836800, filename=\"redditjuldec2019.csv\")     # 8\n",
    "\n",
    "scraping(1577836800, 1593561600, filename=\"redditjanjun2020.csv\")     # 9\n",
    "scraping(1593561600, 1609459200, filename=\"redditjuldec2020.csv\")     # 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Concatenating and writing to a single file\n",
    "datasets = ['redditjanjun2016', 'redditjuldec2016',\n",
    "    'redditjanjun2017', 'redditjuldec2017',\n",
    "    'redditjanjun2018a', 'redditjanjun2018b', 'redditjuldec2018',\n",
    "    'redditjanjun2019', 'redditjuldec2019',\n",
    "    'redditjanjun2020', 'redditjuldec2020']\n",
    "\n",
    "final_df = pd.DataFrame()\n",
    "\n",
    "for elem in datasets:\n",
    "    csv = elem + '.csv'\n",
    "    df = pd.read_csv(csv)\n",
    "    final_df = pd.concat([final_df, df])\n",
    "    \n",
    "final_df.to_csv('intermediate_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Edit variables\n",
    "df = pd.read_csv('intermediate_dataset.csv', index_col = 0)\n",
    "# ID\n",
    "df['id'] = df['id'].astype(str)\n",
    "\n",
    "# Timestamp\n",
    "# transform\n",
    "\n",
    "df['timestamp'] = df['timestamp'].astype(int)\n",
    "\n",
    "def epoch_to_time(elem):\n",
    "    a = datetime.datetime.utcfromtimestamp(elem)\n",
    "    return a\n",
    "\n",
    "def time_to_date(elem):\n",
    "    b = elem.date()\n",
    "    return b\n",
    "\n",
    "df['Time'] = df['timestamp'].apply(epoch_to_time)\n",
    "df['Day'] = df['Time'].apply(time_to_date)\n",
    "\n",
    "# Author\n",
    "df['author'] = df['author'].astype(str)\n",
    "\n",
    "# Score \n",
    "df['score'] = df['score'].astype(int)\n",
    "\n",
    "# Comments\n",
    "df['comments'] = df['comments'].astype(int)\n",
    "\n",
    "# Text\n",
    "df['text'] = df['text'].astype(str)\n",
    "\n",
    "df.to_csv('df_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Zipping the file (done manually)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Coindesk data\n",
    "Powered by Coindesk (https://www.coindesk.com/price/bitcoin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import Modules\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Define the Function \n",
    "# Feed in the YYYY-MM-DD format\n",
    "def get_bitcoin_prices(start, end):\n",
    "    url = 'https://api.coindesk.com/v1/bpi/historical/close.json'\n",
    "    first = '?start=' + start\n",
    "    last = '&end=' + end\n",
    "    full_url = url + first + last\n",
    "    print(full_url)\n",
    "    r = requests.get(full_url)\n",
    "    data = json.loads(r.text)\n",
    "    prices = data['bpi']\n",
    "    return prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.coindesk.com/v1/bpi/historical/close.json?start=2016-01-01&end=2020-12-31\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Write the function and write to .csv file\n",
    "prices = get_bitcoin_prices('2016-01-01', '2020-12-31')\n",
    "\n",
    "df = pd.DataFrame(list(prices.items()), columns=['Date', 'BPI'])\n",
    "df = df.set_index('Date')\n",
    "df.to_csv('bpi.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
